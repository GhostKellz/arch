# ğŸ§  ghostllm

[![Arch-Based](https://img.shields.io/badge/Arch%20Linux-optimized-blue?logo=arch-linux\&logoColor=white)](https://archlinux.org)
[![LLM Powered](https://img.shields.io/badge/Ollama-33B-inference-success?logo=openai\&logoColor=white)](https://ollama.com)
[![VSCode Integrated](https://img.shields.io/badge/VSCode-Continue-blueviolet?logo=visualstudiocode\&logoColor=white)](https://marketplace.visualstudio.com/items?itemName=Continue.continue)
[![LiteLLM API](https://img.shields.io/badge/LiteLLM-Staging-yellow?logo=fastapi\&logoColor=white)](https://github.com/BerriAI/litellm)

> Self-hosted AI tooling and model integration on Arch Linux and Proxmox.

---

## ğŸ“¦ Components

### ğŸ§  LLMs via Ollama (Arch Workstation)

* `deepseek-coder:33b` â€“ for high-performance code completion
* `deepseek-r1:32b` â€“ general-purpose model
* `llama3:8b` â€“ fast fallback / light interaction

All models are pulled locally via [Ollama](https://ollama.com), with GPU acceleration (RTX 4090) for full-speed inference.

---

### ğŸ‘¤ OpenWebUI (Proxmox LXC)

* Hosted remotely in a lightweight LXC container
* Reverse-proxied and accessible over internal VPN
* Primary chat/interaction UI when not using the terminal

---

### ğŸ§¹ VSCode Integration

* [Continue Extension](https://marketplace.visualstudio.com/items?itemName=Continue.continue)
* Fully wired to use `deepseek-coder:33b` as the default model
* Config file managed under `ghostllm/vscode/config.yaml`

---

### ğŸ§ª LiteLLM (Staging)

* A dedicated LXC is being staged for **LiteLLM** to provide:

  * Unified OpenAI-compatible API
  * Per-user limits, API keys, proxy routing
  * Integration target for scripts, clients, and dev tools

---

## ğŸ“ Repo Structure

```bash
ghostllm/
â”œâ”€â”€ vscode/         # Continue config for VSCode
â”œâ”€â”€ ollama/         # Ollama model notes, pull scripts
â”œâ”€â”€ open-webui/     # OpenWebUI setup & reverse proxy
â”œâ”€â”€ nginx/          # (optional) proxy configs
```

---

## âš™ï¸ Platform

* **Host**: Arch Linux (Workstation)
* **Server**: Proxmox (OpenWebUI + LiteLLM LXCs)
* **GPU**: NVIDIA RTX 4090 (CUDA enabled for Ollama)

---

## ğŸ§  Future Plans

* Add Weaviate or Qdrant for embeddings
* Automate model selection via `ghostctl`
* Integrate ghostllm into PhantomBoot + GhostMesh stack

---

